{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Project on Ensemble Techniques\n\n                                                         "},{"metadata":{},"cell_type":"markdown","source":"### About dataset:\n\nThe data is related with direct marketing campaigns of a\nPortuguese banking institution. The marketing campaigns\nwere based on phone calls. Often, more than one contact to\nthe same client was required, in order to access if the product\n(bank term deposit) would be ('yes') or not ('no') subscribed.\n"},{"metadata":{},"cell_type":"markdown","source":"### Domain \nBanking"},{"metadata":{},"cell_type":"markdown","source":"### Context:\nLeveraging customer information is paramount for most\nbusinesses. In the case of a bank, attributes of customers like\nthe ones mentioned below can be crucial in strategizing a\nmarketing campaign when launching a new product."},{"metadata":{},"cell_type":"markdown","source":"### Objective:\nThe classification goal is to predict if the client will subscribe\n(yes/no) a term deposit (variable y)."},{"metadata":{},"cell_type":"markdown","source":"### Learning Outcomes:\n\n* Exploratory Data Analysis\n\n* Preparing the data to train a model\n\n* Training and making predictions using an Ensemble\n  Model\n\n* Tuning an Ensemble model"},{"metadata":{},"cell_type":"markdown","source":"### Import  all neccessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import  KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import confusion_matrix,precision_score,classification_report,f1_score,roc_curve,roc_auc_score,auc,accuracy_score\nfrom sklearn import metrics\nimport pylab as pl\n%matplotlib inline\nimport warnings \nwarnings.simplefilter(\"ignore\")","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df =pd.read_csv(\"../input/bankfullcsv/bank-full.csv\")","execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'bank-full.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fe6267b81a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bank-full.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bank-full.csv'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data types of the attributes\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above output we can see the datatypes of the different attributes.\nIt contains 7 integer type and 10 object type attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the data\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the presence of missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#5 point summary of numerical attributes\ndf.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the presence of outliers\n# Outlier is defined as Data points above or below than 1.5 times the Inter Quartile Range of the data.\nnumerical = ['age','balance','day','duration','campaign','pdays','previous']\nQ1 = df[numerical].quantile(0.25)\nQ3 = df[numerical].quantile(0.75)\nIQR = Q3 - Q1\nout = (df[numerical] < (Q1 - 1.5 * IQR)) | (df[numerical] > (Q3 + 1.5 * IQR))\nout.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 6 columns in the dataset which has outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding unique data\ndf.apply(lambda x: len(x.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Distribution of features"},{"metadata":{},"cell_type":"markdown","source":"Distribution of some of the categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(12,8))\nax=sns.countplot(df['job'],hue=df['Target'],order=df['job'].value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40)\nax.set_title(\"Job Type Vs Target\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the persons belong to the Blue-collar job type .\nHowever more numnber of  persons belonging to management jobs has subscribe to the term deposit.\nThere is also an unknown category in the job type which needs to be replaced."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(12,8))\nax=sns.countplot(df['marital'],hue=df['Target'],order=df['marital'].value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40)\nax.set_title(\"Marital Status Vs Target\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More than half of the customers are married. Less no of divorced customers has said yes for term deposit."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(10,8))\nax=sns.countplot(df['education'],hue=df['Target'],order=df['education'].value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40)\nax.set_title(\"Education Vs Target\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the contacted List of Customer have secondary level of education."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(10,8))\nax=sns.countplot(df['default'],hue=df['Target'],order=df['default'].value_counts().index)\nax.set_title(\"Credit in default Vs Target\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of the customer have credit in default."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(10,8))\nax=sns.countplot(df['housing'],hue=df['Target'],order=df['housing'].value_counts().index)\nax.set_title(\"Housing Vs Target\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more number of customers who has housing loan, however customers that dont have housing loan has more number in saying yes for term deposit."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(10,8))\nax=sns.countplot(df['loan'],hue=df['Target'],order=df['loan'].value_counts().index)\nax.set_title(\" Personal Loan Vs Target\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of the customers has personal loan"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(12,6))\nax=sns.countplot(df['contact'],hue=df['Target'],order=df['contact'].value_counts().index)\nax.set_title(\"Contact communication type Vs Target\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the customers has been contacted via cellular communication type. There are many unknown communication type in the dataset.However the type of communication does not effect the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(12,6))\nax=sns.countplot(df['month'],hue=df['Target'],order=df['month'].value_counts().index)\nax.set_title(\"Last contact month Vs Target\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the customers has been contacted in the month of may."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(12,6))\nax=sns.countplot(df['poutcome'],hue=df['Target'],order=df['poutcome'].value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40)\nax.set_title(\"Previous campaign Outcome Vs Target\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More than 75% of the values in the column are unknown and we cannot drop those values as there will be huge data loss.\nIt is also observed from the graph that customers who are successfully enrolled from the previous campaign are more likely to say yes for this current campaign also."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='campaign', y='duration', data=df, fit_reg=False, hue='Target',height=8)\nplt.xlabel('Number of Calls')\nplt.ylabel('Duration of Calls (Seconds)')\nplt.title('The Relationship between the Number and Duration of Calls (with Response Result)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is observed from the graph that as the call duration increases there is increment in number of saying yes.\nHowever if the number of calls increases it is more likely that the customers will say no."},{"metadata":{},"cell_type":"markdown","source":"Distribution of numerical features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numerical].hist(figsize=(15,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Age - From the graph it is observed that the Age column has slight right skewed distribution.\n* Balance - The distribution is highly right skewed and most of the customers have balance less than 5000 euros\n* Campaign - The distribution is highly right skewed.\n* Day - The distribution shows that most of the customers contact in the mid on the month.\n* Duration - The distribution is highly right skewed.\n* pdays - It is observed that less number of customers have been contacted by the bank.Distribution is hoghly skewed.\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of Target Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Target\").agg({'Target': 'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(figsize=(8,7) )\ndf['Target'].value_counts(sort=True).plot(kind='pie',autopct='%1.1f%%', fontsize= 20,startangle=130)\nplt.legend(['Rejected ','Accepted '])\nplt.title('Percentage of customers for Accepting / Rejecting the offer')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph it is observed that dataset is highly baised.Only 11.7% customers had accepted the loan."},{"metadata":{},"cell_type":"markdown","source":"##### Corelation between columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot for Visualising the correlation between variables and Target Column.\n\nfig,ax = plt.subplots( figsize=(16,8) )\nsns.heatmap(df.corr(),annot=True)\nplt.title('Heatmap for Correlation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparation of data "},{"metadata":{},"cell_type":"markdown","source":"#### Get rid of missing or unknown values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"List of unique values in poutcome \\n\", df['poutcome'].value_counts())\nprint(\"\\nList of unique values in education \\n\", df['education'].value_counts())\nprint(\"\\nList of unique values in job \\n\", df['job'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 36959 unknown values in poutcome column which is 81.74% of the total.If we drop the values there will be significant loss in data.Hence we keep the data and assumed that unknown is a category for the particular feature.\nThere are 1840 other values which we can replace to unknown type."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['poutcome'].replace('other','unknown',inplace=True)\nprint(df['poutcome'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 1857 unknown values in Job column and 288 unknown values in education column,which is around 4% and 0.6% of the data.\nHence we can drop those values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df['job']=='unknown'].index,inplace=True,axis=0)\ndf.drop(df[df['education']=='unknown'].index,inplace=True,axis=0)\nprint(\"Unique values in job\",df['job'].unique())\nprint(\"Unique values in eduaction \",df['education'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Converting Categorical features into numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_column = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month','poutcome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded = pd.get_dummies(df,columns=categorical_column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Dropping the duration column from the data as it is highly affects the output.Thus, this input should\nonly be included for benchmark purposes and should be\ndiscarded if the intention is to have a realistic predictive\nmodel. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded.drop(\"duration\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Separating the target column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded['Target'] = df_encoded['Target'].map({'yes': 1, 'no': 0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_encoded[\"Target\"]\nX = df_encoded.drop(\"Target\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spliiting the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.30, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scaling the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc= StandardScaler()\nscaledX_train = sc.fit_transform(X_train)\nscaledX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Applyling different Classification Models"},{"metadata":{},"cell_type":"markdown","source":"* Classification models used are Logistic Regression and Decision Trees.\n* KNN is not used because k-NN doesn’t perform well on imbalanced data. And it is a highly imbalance dataset.\n* SVM model is difficult to understand and interpret by human beings unlike Decision Trees.And here we are focusing on ensemble models more. All ensemble algorithm are used for training the model."},{"metadata":{},"cell_type":"markdown","source":"##### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=1)\nlr.fit(scaledX_train,y_train)\nlr_pred = lr.predict(scaledX_test)\nlr_training = lr.score(scaledX_train,y_train)\nlr_testing = lr.score(scaledX_test,y_test)\nlr_precision = precision_score(y_test,lr_pred)\nlr_f1 = f1_score(y_test,lr_pred)\nprint(\"Training Accuracy :\", lr_training)\nprint(\"Testing Accuracy :\",lr_testing )\nprint(\"Precision :\",lr_precision )\nprint(\"F1 Score: \",lr_f1 )\nprint('Confusion Matrix - Logistic Regression :\\n\\n',confusion_matrix(y_test, lr_pred) )\n#tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  We have a uneven class distribution so accuracy and presicion would not be good metrics to compare.\n* So F1 score can be used for comparing the models "},{"metadata":{},"cell_type":"markdown","source":"  Logistic Regression - \n* Training and testing accuracy are good, however there high number of False Positive.\n* F1 score this model is 0.28"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1 Sccore\\n :',classification_report(y_test, lr_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating CART model with max_depth = 5\n\ndt = DecisionTreeClassifier(criterion='entropy',max_depth=1)\ndt.fit(scaledX_train,y_train)\ndt_pred = dt.predict(scaledX_test)\ndt_training = dt.score(scaledX_train,y_train)\ndt_testing = dt.score(scaledX_test,y_test)\ndt_precision = precision_score(y_test,dt_pred)\ndt_f1 = f1_score(y_test,dt_pred)\nprint(\"Traing Accuracy :\", dt_training)\nprint(\"Testing Accuracy :\",dt_testing )\nprint(\"F1 Score: \",dt_f1 )\nprint('Decision Tree Confusion matrix :\\n\\n',confusion_matrix(y_test, dt_pred) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Trees \n* Good testing and training cuuracy\n* False Positive reduced compared to other above two models(still high)\n* F1 score is 0.25"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1 Score\\n :',classification_report(y_test, dt_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models on Ensemble "},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(criterion='entropy',max_depth=50,n_estimators=50)\nrf.fit(scaledX_train,y_train)\nrf_pred = rf.predict(scaledX_test)\nrf_training = rf.score(scaledX_train,y_train)\nrf_testing = rf.score(scaledX_test,y_test)\nrf_precision = precision_score(y_test,rf_pred)\nrf_f1 = f1_score(y_test,rf_pred)\nprint(\"Traing Accuracy :\", rf_training)\nprint(\"Testing Accuracy :\",rf_testing )\nprint(\"F1 Score: \",dt_f1 )\nprint('Random Forest Confusion matrix :\\n\\n',confusion_matrix(y_test, rf_pred) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest\n* Testing and training accuracy are good.\n* False positive is comparitively lower than classification models\n* F1 score is 0.29"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1 Score\\n :',classification_report(y_test, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"bg = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=500,bootstrap=True,max_samples=100)\nbg.fit(scaledX_train,y_train)\nbg_pred = bg.predict(scaledX_test)\nbg_training = bg.score(scaledX_train,y_train)\nbg_testing = bg.score(scaledX_test,y_test)\nbg_precision = precision_score(y_test,bg_pred)\nbg_f1 = f1_score(y_test,bg_pred)\nprint(\"Traing Accuracy :\", bg_training)\nprint(\"Testing Accuracy :\",bg_testing )\nprint(\"F1 Score: \",bg_f1 )\nprint('Bagging Classifier Confusion matrix :\\n\\n',confusion_matrix(y_test, bg_pred) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bagging Classifier\n* Testing and training accuracy are good.\n* False positive is comparitively lower than classification models\n* F1 score is 0.23"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1 Score\\n :',classification_report(y_test, bg_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),n_estimators=10,learning_rate=0.5)\nada.fit(scaledX_train,y_train)\nada_pred = ada.predict(scaledX_test)\nada_training = ada.score(scaledX_train,y_train)\nada_testing = ada.score(scaledX_test,y_test)\nada_f1 = f1_score(y_test,ada_pred)\nada_precision = precision_score(y_test,ada_pred)\nprint(\"Traing Accuracy :\", ada_training)\nprint(\"Testing Accuracy :\",ada_testing )\nprint(\"F1 Score: \",ada_f1 )\nprint('AdaBoost Classifier Confusion matrix :\\n\\n',confusion_matrix(y_test, ada_pred) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AdaBoost Classifier\n* Testing and training accuracy are good.\n* High number of False positives (more than bagging classifier)\n* F1 score is 0.23"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1 Score\\n :',classification_report(y_test, ada_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GradientBoost Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc = GradientBoostingClassifier(learning_rate=0.02,n_estimators=65)\ngbc.fit(scaledX_train,y_train)\ngbc_pred = gbc.predict(scaledX_test)\ngbc_training = gbc.score(scaledX_train,y_train)\ngbc_testing = gbc.score(scaledX_test,y_test)\ngbc_f1 = f1_score(y_test,gbc_pred)\ngbc_precision = precision_score(y_test,gbc_pred)\nprint(\"Traing Accuracy :\", gbc_training)\nprint(\"Testing Accuracy :\",gbc_testing )\nprint(\"F1 Score: \",gbc_f1 )\nprint('AdaBoost Classifier Confusion matrix :\\n\\n',confusion_matrix(y_test, gbc_pred) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AdaBoost Classifier\n* Testing and training accuracy are good.\n* False positive is comparitively low (lowest among other models)\n* F1 score is 0.20 "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1 Score\\n :',classification_report(y_test, gbc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC for different models"},{"metadata":{},"cell_type":"markdown","source":"### ROC for ensemble models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC for Random Forest\nrf_prob = rf.predict_proba(X_test)\nfpr,tpr,thresh=roc_curve(y_test,rf_prob[:,1])\nauc1 = auc(fpr,tpr)\nprint(\"Area under the curve for Random Forest \", auc1)\n\n#ROC for Bagging\nbg_prob = bg.predict_proba(X_test)\nfpr1,tpr1,thresh1=roc_curve(y_test,bg_prob[:,1])\nauc2 = auc(fpr1,tpr1)\nprint(\"Area under the curve for Bagging \", auc2)\n\n#ROC for AdaBoost\nada_prob = ada.predict_proba(X_test)\nfpr2,tpr2,thresh2=roc_curve(y_test,ada_prob[:,1])\nauc3 = auc(fpr2,tpr2)\nprint(\"Area under the curve for AdaBoost \", auc3)\n\n#ROC for GradientBoost\ngbc_prob = gbc.predict_proba(X_test)\nfpr3,tpr3,thresh3=roc_curve(y_test,gbc_prob[:,1])\nauc4 = auc(fpr3,tpr3)\nprint(\"Area under the curve for  Gradient \", auc4)\n\nlr_prob = lr.predict_proba(X_test)\nlr_fpr,lr_tpr,lr_thresh=roc_curve(y_test,lr_prob[:,1])\nlr_auc = auc(lr_fpr,lr_tpr)\nprint(\"Area under the curve for  Logistic Regression \", lr_auc)\n\ndt_prob = dt.predict_proba(X_test)\ndt_fpr,dt_tpr,dt_thresh=roc_curve(y_test,dt_prob[:,1])\ndt_auc = auc(dt_fpr,dt_tpr)\nprint(\"Area under the curve for Decision Trees \", dt_auc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the ROC curve \nplt.clf()\nfig, ax= plt.subplots(nrows = 2, ncols = 2, figsize = (12,10))\nax[0,0].plot(fpr, tpr, label='AUC area = %0.2f' % auc1)\nax[0,0].plot([0, 1], [0, 1], 'k--')\nax[0,0].set_xlabel('False Positive Rate')\nax[0,0].set_ylabel('True Positive Rate')\nax[0,0].set_title('ROC for Random Forest')\nax[0,0].legend(loc=\"lower right\")\n\nax[0,1].plot(fpr1, tpr1, label='AUC = %0.2f' % auc2)\nax[0,1].plot([0, 1], [0, 1], 'k--')\nax[0,1].set_xlabel('False Positive Rate')\nax[0,1].set_ylabel('True Positive Rate')\nax[0,1].set_title('ROC for Bagging')\nax[0,1].legend(loc=\"lower right\")\n\nax[1,0].plot(fpr2, tpr2, label='AUC = %0.2f' % auc3)\nax[1,0].plot([0, 1], [0, 1], 'k--')\nax[1,0].set_xlabel('False Positive Rate')\nax[1,0].set_ylabel('True Positive Rate')\nax[1,0].set_title('ROC for AdaBoost')\nax[1,0].legend(loc=\"lower right\")\n\nax[1,1].plot(fpr3, tpr3, label='AUC = %0.2f' % auc4)\nax[1,1].plot([0, 1], [0, 1], 'k--')\nax[1,1].set_xlabel('False Positive Rate')\nax[1,1].set_ylabel('True Positive Rate')\nax[1,1].set_title('ROC Gradient')\nax[1,1].legend(loc=\"lower right\")\n\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparining different models"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_compare = pd.DataFrame([[lr_training,lr_testing,lr_precision,lr_f1,lr_auc],[dt_training,dt_testing,dt_precision,dt_f1,dt_auc],\n                          [rf_training,rf_testing,rf_precision,rf_f1,auc1],[bg_training,bg_testing,bg_precision,bg_f1,auc2],\n                          [ada_training,ada_testing,ada_precision,ada_f1,auc3],[gbc_training,gbc_testing,gbc_precision,gbc_f1,auc4]],\n    columns=['Training Accuracy','Testing Accuracy','Precision Score','F1Score','AUC'],\n                       index=['Logistic Regression','DecisionTrees',\n                              'RandomForest','BaggingClassifier','AdaBoost','GradientBoosting'])\ndf_compare","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Key Observations"},{"metadata":{},"cell_type":"markdown","source":"* As the dataset highly baised most of the algorithm are having training accuracy in range of (88-89.9)%.\n* All of the models are not trained equally to both set of classes(majority is class 0 as not accepting the term deposit). So     the models are performing well for class 0 ie not accepting the loan and not able to perform well on the positive class ie       class 1.This is the main reason for inferior precison as well as F1 scores.\n* Trianing accuracy of the models are somehow large than testing accuracy."},{"metadata":{},"cell_type":"markdown","source":"#### Some more observations"},{"metadata":{},"cell_type":"markdown","source":"* We cannot compare the models on the basis of high Accuracy,Presion or Recall score as it is a highly biased dataset.\n* Other metrics that can be used for comparision are - F1 Score , AUC .\n* Also according to the given context/ objective of the project it can be say that reducing the values of False Positve should     be given more importance.So it can also be one of the factors for comparision. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 3 models with Highest testing accuracy are - \ndf_compare.sort_values(ascending=False,by=['Testing Accuracy'])['Testing Accuracy'].head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 3 models with highest F1 Score are \ndf_compare.sort_values(ascending=False,by=['F1Score'])['F1Score'].head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 3 Models with highest AUC score\ndf_compare.sort_values(ascending=False,by=['AUC'])['AUC'].head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix of the models sorted according to False Postive scores in ascending order\n\nprint(\"Gradient Boost Classifier\\n\",confusion_matrix(y_test, gbc_pred))\nprint(\"Bagging Classifier \\n\",confusion_matrix(y_test, bg_pred))\nprint(\"AdaBoost Classifier\\n\",confusion_matrix(y_test, ada_pred))\nprint(\"Decision Tree Classifier \\n\",confusion_matrix(y_test, dt_pred))\nprint(\"Random Forest Classifier \\n\",confusion_matrix(y_test, rf_pred))\nprint(\"Logistic Regression \\n\",confusion_matrix(y_test, lr_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"* We cannot commnent on any one of the models as performing best than others ones, depending on the metrics such as \n  F1 Score  Random Forest has highest score, if we consider the AUC for comparision then GradientBoost & AdaBoost has             highest AUC value. GradientBoost also has less number of False positive compare to other models.\n* We can conclude that ensembles models are performing better than the single classification models.\n* We can improve the accuracy of the models if we can somehow balance the dataset using sampling techniques such as upsampling     and downsampling."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}